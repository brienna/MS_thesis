{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# doc2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import glob\n",
    "import gensim\n",
    "import os\n",
    "import multiprocessing as mp\n",
    "from timeit import default_timer as timer\n",
    "from datetime import timedelta\n",
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.model_selection import GridSearchCV, ParameterGrid\n",
    "from scipy.spatial.distance import cosine as cosine_distance\n",
    "import re\n",
    "from sklearn.base import BaseEstimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Volumes/BRIENNAKH/Thesis/notebooks/abstracts/233.npy'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "FILE_DIR = os.path.dirname(os.path.abspath('__file__'))\n",
    "sys.path.append(FILE_DIR)\n",
    "datapath = os.path.join(FILE_DIR, 'abstracts/' + 'numtoken' + '/*.npy')\n",
    "loaded = os.path.join(FILE_DIR, 'abstracts/' + '233' + '.npy')\n",
    "loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 1000\n"
     ]
    }
   ],
   "source": [
    "data = np.array(glob.glob('/Volumes/BRIENNAKH/Thesis/data/2020_06_12_abstract_tokens/numtoken/*.npy'))[:1000]\n",
    "print('Number of documents: ' + str(len(data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write the iterator that will load and then close each document, one at a time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocumentIterator(object):\n",
    "    def load_documents(self):\n",
    "        for filename in self.filenames: \n",
    "            loaded_file = np.load(filename)\n",
    "            tag = os.path.splitext(os.path.basename(filename))[0]\n",
    "            abstract = gensim.models.doc2vec.TaggedDocument(words=loaded_file, tags=[tag])\n",
    "            try:\n",
    "                yield abstract\n",
    "            except Exception as e:\n",
    "                print('Error!' + str(e))\n",
    "            finally:\n",
    "                del loaded_file\n",
    "                # print('Closed ' + tag)\n",
    "                \n",
    "    def __init__(self, filenames):\n",
    "        self.filenames = filenames\n",
    "        #self.documents = self.load_documents()\n",
    "\n",
    "    def __iter__(self):\n",
    "        self.documents = self.load_documents() # Reset the iterator\n",
    "        return self\n",
    "    \n",
    "    def __next__(self):\n",
    "        abstract = next(self.documents)\n",
    "        return abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write GridSearch code, troubleshooting on mini corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confirm mini corpus trains quickly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DocumentIterator' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-8f47f3cd9fff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmini_corpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDocumentIterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilenames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m model = gensim.models.doc2vec.Doc2Vec(vector_size=100,\n\u001b[1;32m      3\u001b[0m                                       \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.025\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                                       \u001b[0mmin_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                                       \u001b[0mmin_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'DocumentIterator' is not defined"
     ]
    }
   ],
   "source": [
    "mini_corpus = DocumentIterator(filenames[:1000])\n",
    "model = gensim.models.doc2vec.Doc2Vec(vector_size=100,\n",
    "                                      alpha=0.025,\n",
    "                                      min_alpha=0.001,\n",
    "                                      min_count=1,\n",
    "                                      workers=mp.cpu_count(),\n",
    "                                      dm=1,\n",
    "                                      epochs=10)\n",
    "model.build_vocab(mini_corpus)\n",
    "model.train(mini_corpus, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "model.save('/Volumes/BRIENNAKH/Thesis/results/2020_06_19_doc2vec_1000_docs/model.d2v')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select parameters to optimize. \n",
    "\n",
    "- dm \n",
    "    - 0: \"distributed bag of words\" (PV-DBOW)\n",
    "    - 1: \"distributed memory\" (PV-DM)\n",
    "- hs \n",
    "    - 0: \"hierarchical softmax\" used for model training\n",
    "    - 1: negative sampling will be used for model training (if negative is non-zero too)\n",
    "    \n",
    "\"By following a grid-search strategy, we optimized six parameters to train more than 1,900 models.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Doc2VecModel(BaseEstimator):\n",
    "    '''The Estimator that will be used for GridSearch.\n",
    "    In __init__ we take parameters for a specific model,\n",
    "    then the GridSearch will call fit on this model.'''\n",
    "    \n",
    "    def __init__(self, dm=1, vector_size=100, window=1):\n",
    "        '''Must match all parameters in my param dict.'''\n",
    "        self.model = None\n",
    "        self.vector_size = vector_size\n",
    "        self.window = window\n",
    "        self.dm = dm\n",
    "\n",
    "    def fit(self, train_set, triplets_path):\n",
    "        '''y = None cuz this is unsupervised training.'''\n",
    "        self.triplets_path = triplets_path\n",
    "        self.model = gensim.models.doc2vec.Doc2Vec(vector_size=self.vector_size, \n",
    "                                                   window=self.window, \n",
    "                                                   dm=self.dm, \n",
    "                                                   epochs=10,\n",
    "                                                   alpha=0.025, \n",
    "                                                   min_alpha=0.001)\n",
    "        tagged_docs = DocumentIterator(train_set)\n",
    "        self.model.build_vocab(tagged_docs)\n",
    "        self.model.train(docs, total_examples=self.model.corpus_count, epochs=self.model.epochs)\n",
    "        return self\n",
    "\n",
    "def evaluate(self, model):\n",
    "    '''Calculates averaged accuracy of all given triplets.\n",
    "    [accuracy that ab is more similar than ac, ab more similar than bc]'''\n",
    "    accuracies = [0, 0]\n",
    "    triplets = np.load(model.triplets_path)\n",
    "    for triplet in triplets:\n",
    "        accuracy = calculate_accuracy(model, triplet)\n",
    "        accuracies[0] += accuracy[0]\n",
    "        accuracies[1] += accuracy[1]\n",
    "    return [accuracies[0]/len(triplets), accuracies[1]/len(triplets)]\n",
    "    \n",
    "def scorer(self, estimator):\n",
    "    accuracy = evaluate(estimator.model)\n",
    "    return max(accuracy)\n",
    "\n",
    "def get_vector(model, doc_id):\n",
    "    '''Takes a Pandas row for a paper.'''\n",
    "    loaded = np.load('/Volumes/BRIENNAKH/Thesis/data/2020_06_09_abstract_tokens/' + doc_id + '.npy')\n",
    "    return model.infer_vector(loaded)\n",
    "\n",
    "def calculate_accuracy(model, triplet):\n",
    "    '''Calculates accuracy of one given triplet using two formulas.\n",
    "    [ab > ac, ab > bc]. 1= yes, 0 = no.'''\n",
    "    a_vector = get_vector(model, triplet[0])\n",
    "    b_vector = get_vector(model, triplet[1])\n",
    "    c_vector = get_vector(model, triplet[2])\n",
    "    ab_dist = cosine_distance(a_vector, b_vector)\n",
    "    ac_dist = cosine_distance(a_vector, c_vector)\n",
    "    bc_dist = cosine_distance(b_vector, c_vector)\n",
    "    if ab_dist > ac_dist and ab_dist > bc_dist:\n",
    "        return [1, 1]\n",
    "    elif ab_dist > ac_dist and ab_dist < bc_dist:\n",
    "        return [1, 0]\n",
    "    elif ab_dist < ac_dist and ab_dist > bc_dist:\n",
    "        return [0, 1]\n",
    "    else:\n",
    "        return [0, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up GridSearch. Helpful [tutorial](https://medium.com/datadriveninvestor/an-introduction-to-grid-search-ff57adcc0998)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'corpus': 'numtoken', 'dm': 0, 'hs': 0, 'vector_size': 100}\n",
      "dict_values(['numtoken', 0, 0, 100])\n",
      "{'corpus': 'numtoken', 'dm': 0, 'hs': 0, 'vector_size': 200}\n",
      "dict_values(['numtoken', 0, 0, 200])\n",
      "{'corpus': 'numtoken', 'dm': 0, 'hs': 0, 'vector_size': 300}\n",
      "dict_values(['numtoken', 0, 0, 300])\n",
      "{'corpus': 'numtoken', 'dm': 0, 'hs': 1, 'vector_size': 100}\n",
      "dict_values(['numtoken', 0, 1, 100])\n",
      "{'corpus': 'numtoken', 'dm': 0, 'hs': 1, 'vector_size': 200}\n",
      "dict_values(['numtoken', 0, 1, 200])\n",
      "{'corpus': 'numtoken', 'dm': 0, 'hs': 1, 'vector_size': 300}\n",
      "dict_values(['numtoken', 0, 1, 300])\n",
      "{'corpus': 'numtoken', 'dm': 1, 'hs': 0, 'vector_size': 100}\n",
      "dict_values(['numtoken', 1, 0, 100])\n",
      "{'corpus': 'numtoken', 'dm': 1, 'hs': 0, 'vector_size': 200}\n",
      "dict_values(['numtoken', 1, 0, 200])\n",
      "{'corpus': 'numtoken', 'dm': 1, 'hs': 0, 'vector_size': 300}\n",
      "dict_values(['numtoken', 1, 0, 300])\n",
      "{'corpus': 'numtoken', 'dm': 1, 'hs': 1, 'vector_size': 100}\n",
      "dict_values(['numtoken', 1, 1, 100])\n",
      "{'corpus': 'numtoken', 'dm': 1, 'hs': 1, 'vector_size': 200}\n",
      "dict_values(['numtoken', 1, 1, 200])\n",
      "{'corpus': 'numtoken', 'dm': 1, 'hs': 1, 'vector_size': 300}\n",
      "dict_values(['numtoken', 1, 1, 300])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "params = {'dm': [0, 1],\n",
    "          'vector_size': [100, 200, 300], \n",
    "          'corpus': ['numtoken'],\n",
    "          'hs': [0, 1]}\n",
    "\n",
    "for i in ParameterGrid(params):\n",
    "    print(i)\n",
    "    print(i.values())\n",
    "    \n",
    "len(list(ParameterGrid(params)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:c.ids :[0, 1, 2, 3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time HH:MM:SS: 0:00:26.583568\n"
     ]
    }
   ],
   "source": [
    "from joblib import Parallel, parallel_backend, register_parallel_backend, delayed\n",
    "from ipyparallel.joblib import IPythonParallelBackend\n",
    "from ipyparallel import Client\n",
    "import sys \n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from thiswillwork import train_and_evaluate\n",
    "\n",
    "start = timer()\n",
    "params = {\n",
    "    'dm': [0, 1],\n",
    "    'vector_size': [100, 300]\n",
    "}\n",
    "\n",
    "FILE_DIR = os.path.dirname(os.path.join(os.path.abspath(''), 'notebooks'))\n",
    "sys.path.append(FILE_DIR)\n",
    "\n",
    "profile = 'ipy_profile'\n",
    "c = Client(profile=profile)\n",
    "\n",
    "triplets = np.load('/s/BRIENNAKH/Thesis/results/2020_06_19_doc2vec_1000_docs/100_triplets.npy')\n",
    "\n",
    "# Make sure that each engine is running in the right \n",
    "# working directory to access the custom function(s)\n",
    "c[:].map(os.chdir, [FILE_DIR]*len(c))\n",
    "logging.info(\"c.ids :{0}\".format(str(c.ids)))\n",
    "bview = c.load_balanced_view()\n",
    "register_parallel_backend('ipyparallel', lambda : IPythonParallelBackend(view=bview))\n",
    "\n",
    "with parallel_backend('ipyparallel'):\n",
    "    scores = Parallel(n_jobs=len(c))(delayed(train_and_evaluate)(data, triplets, p) for p in ParameterGrid(params))\n",
    "    scores_formatted = {k: v for d in scores for k, v in d.items()}\n",
    "    with open('scores.pkl', 'wb') as f:\n",
    "        pickle.dump(scores_formatted, f, pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "end = timer()\n",
    "print (\"Execution time HH:MM:SS:\",timedelta(seconds=end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{1: {'parameters': {'corpus': 'numtoken', 'dm': 1, 'vector_size': 200}, 'score': 0.45}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'parameters': {'corpus': 'numtoken', 'dm': 1, 'vector_size': 1000}, 'score': 0.44}\n",
      "dict_keys([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19])\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "with open('/Volumes/BRIENNAKH/Thesis/src/2020_06_20_doc2vec_slurm_pkg/scores.pkl', 'rb') as f:\n",
    "    scores = pickle.load(f)\n",
    "    best_parameters = scores[max(scores.keys())]\n",
    "    print(best_parameters)\n",
    "    print(scores.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>corpus</th>\n",
       "      <th>dm</th>\n",
       "      <th>vector_size</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>numtoken</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>numtoken</td>\n",
       "      <td>0</td>\n",
       "      <td>200</td>\n",
       "      <td>0.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>numtoken</td>\n",
       "      <td>0</td>\n",
       "      <td>300</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>numtoken</td>\n",
       "      <td>0</td>\n",
       "      <td>400</td>\n",
       "      <td>0.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>numtoken</td>\n",
       "      <td>0</td>\n",
       "      <td>500</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>numtoken</td>\n",
       "      <td>0</td>\n",
       "      <td>600</td>\n",
       "      <td>0.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>numtoken</td>\n",
       "      <td>0</td>\n",
       "      <td>700</td>\n",
       "      <td>0.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>numtoken</td>\n",
       "      <td>0</td>\n",
       "      <td>800</td>\n",
       "      <td>0.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>numtoken</td>\n",
       "      <td>0</td>\n",
       "      <td>900</td>\n",
       "      <td>0.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>numtoken</td>\n",
       "      <td>0</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>numtoken</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>0.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>numtoken</td>\n",
       "      <td>1</td>\n",
       "      <td>200</td>\n",
       "      <td>0.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>numtoken</td>\n",
       "      <td>1</td>\n",
       "      <td>300</td>\n",
       "      <td>0.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>numtoken</td>\n",
       "      <td>1</td>\n",
       "      <td>400</td>\n",
       "      <td>0.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>numtoken</td>\n",
       "      <td>1</td>\n",
       "      <td>500</td>\n",
       "      <td>0.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>numtoken</td>\n",
       "      <td>1</td>\n",
       "      <td>600</td>\n",
       "      <td>0.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>numtoken</td>\n",
       "      <td>1</td>\n",
       "      <td>700</td>\n",
       "      <td>0.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>numtoken</td>\n",
       "      <td>1</td>\n",
       "      <td>800</td>\n",
       "      <td>0.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>numtoken</td>\n",
       "      <td>1</td>\n",
       "      <td>900</td>\n",
       "      <td>0.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>numtoken</td>\n",
       "      <td>1</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.44</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      corpus  dm  vector_size score\n",
       "0   numtoken   0          100   0.4\n",
       "1   numtoken   0          200  0.38\n",
       "2   numtoken   0          300   0.4\n",
       "3   numtoken   0          400  0.39\n",
       "4   numtoken   0          500   0.4\n",
       "5   numtoken   0          600  0.42\n",
       "6   numtoken   0          700  0.42\n",
       "7   numtoken   0          800  0.41\n",
       "8   numtoken   0          900  0.44\n",
       "9   numtoken   0         1000  0.42\n",
       "10  numtoken   1          100  0.45\n",
       "11  numtoken   1          200  0.45\n",
       "12  numtoken   1          300  0.43\n",
       "13  numtoken   1          400  0.46\n",
       "14  numtoken   1          500  0.44\n",
       "15  numtoken   1          600  0.46\n",
       "16  numtoken   1          700  0.45\n",
       "17  numtoken   1          800  0.43\n",
       "18  numtoken   1          900  0.44\n",
       "19  numtoken   1         1000  0.44"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_df = pd.DataFrame(scores).transpose()\n",
    "scores_series = scores_df['score']\n",
    "scores_df = scores_df['parameters'].apply(pd.Series)\n",
    "scores_df['score'] = scores_series\n",
    "scores_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "triplets_path = '/Volumes/BRIENNAKH/Thesis/results/2020_06_19_doc2vec_1000_docs/100_triplets.npy'\n",
    "\n",
    "\n",
    "grid = GridSearchCV(estimator=Doc2VecModel(),\n",
    "                   param_grid=params,\n",
    "                   cv=5, \n",
    "                   scoring=scorer, \n",
    "                   verbose=0, \n",
    "                   n_jobs=-1)\n",
    "\n",
    "grid_result = grid.fit(train_set, triplets_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D2VTransformer(DocumentIterator(train_set)).get_params().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model. Do I need to include seed if I'm doing gridsearch over hyperparameters? Ask this question on google groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_result.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_result.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = DocumentIterator(filenames)\n",
    "model = gensim.models.doc2vec.Doc2Vec(vector_size=300,\n",
    "                               alpha=0.025,\n",
    "                               min_alpha=0.001, \n",
    "                               min_count=1,\n",
    "                workers=mp.cpu_count(),\n",
    "                dm=1,\n",
    "                epochs=10)\n",
    "\n",
    "# Build vocabulary\n",
    "start = timer()\n",
    "model.build_vocab(docs)\n",
    "end = timer()\n",
    "print('Built vocabulary!')\n",
    "print (\"Execution time HH:MM:SS:\",timedelta(seconds=end-start))\n",
    "\n",
    "# Train the model\n",
    "start = timer()\n",
    "model.train(docs, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "model.save('/Volumes/BRIENNAKH/Thesis/results/2020_06_13_doc2vec/d2v.model')\n",
    "print('Model Saved!')\n",
    "end = timer()\n",
    "print (\"Execution time HH:MM:SS:\",timedelta(seconds=end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('/Volumes/BRIENNAKH/Thesis/results/2020_06_13_doc2vec/d2v.model')\n",
    "print('Model Saved!')\n",
    "end = timer()\n",
    "print (\"Execution time HH:MM:SS:\",timedelta(seconds=end-start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are the inferred vectors close to the actual ones?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_id = random.choice(list(model.docvecs.doctags))\n",
    "doc = np.load('/Volumes/BRIENNAKH/Thesis/data/2020_06_09_abstract_tokens/' + doc_id + '.npy')\n",
    "inferred_docvec = model.infer_vector(doc)\n",
    "print(doc_id)\n",
    "print('%s:\\n %s' % (model, model.docvecs.most_similar([inferred_docvec], topn=5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analogy_results = model.wv.evaluate_word_analogies('/Volumes/BRIENNAKH/Thesis/data/2020_06_12_analogies/analogies.txt')\n",
    "analogy_results[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Words in model: ' + str(len(model.wv.vocab.keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.most_similar('NASA')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cannot use GPU. https://github.com/RaRe-Technologies/gensim/issues/449 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.Doc2Vec.load('../results/2020_06_10_doc2vec_model/d2v.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.00149401,  0.00273189, -0.00425196,  0.00217884,  0.00158152,\n",
       "       -0.00231829,  0.00190731,  0.00491803, -0.00284187, -0.00442933,\n",
       "       -0.0044723 , -0.00283791,  0.00352604,  0.0020828 , -0.00205341,\n",
       "       -0.00454024, -0.00439921, -0.00181873, -0.00223137,  0.00359157,\n",
       "       -0.0042825 , -0.00327853,  0.00307286, -0.00119451,  0.00275379,\n",
       "        0.00476028, -0.00437622, -0.00143195, -0.00209123,  0.0030231 ,\n",
       "        0.00207966, -0.00406045,  0.00305885,  0.00239352,  0.00127178,\n",
       "       -0.00244026, -0.00463173, -0.00292976,  0.00181851,  0.00249871,\n",
       "        0.00096328, -0.00249196, -0.00111651,  0.00109859, -0.00014503,\n",
       "        0.00373352,  0.00173389, -0.00496073,  0.00291012,  0.00439866,\n",
       "       -0.00458495,  0.00036439, -0.00448882, -0.00445445, -0.00042387,\n",
       "        0.00222825,  0.00472964,  0.0026782 ,  0.00313185, -0.00106612,\n",
       "        0.00052141,  0.00268776, -0.00017095,  0.00499439, -0.00288416,\n",
       "       -0.00314789, -0.00411559, -0.0019212 ,  0.00353793,  0.00086114,\n",
       "        0.00103501, -0.00022371,  0.00428655, -0.00047708, -0.00169852,\n",
       "        0.00036144,  0.00201536,  0.00150073, -0.00084949, -0.00486644,\n",
       "        0.00269233,  0.0038954 , -0.00124727, -0.00338716, -0.00120205,\n",
       "       -0.00305123, -0.00428835, -0.00289294, -0.00306578,  0.00194288,\n",
       "        0.0041344 ,  0.0011648 ,  0.00363643, -0.00032717, -0.00067418,\n",
       "        0.00497832, -0.00076254, -0.00135511, -0.00303392, -0.00461136],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.docvecs['1508.02437']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
