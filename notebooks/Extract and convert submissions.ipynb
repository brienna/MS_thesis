{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert preprints \n",
    "\n",
    "Contents:\n",
    "1. Introduction\n",
    "2. Extract and convert preprints\n",
    "2. Identify preprints\n",
    "3. Convert preprints\n",
    "4. Appendix: Request a submission from arXiv API\n",
    "\n",
    "## 1. Introduction \n",
    "\n",
    "In this notebook, we convert preprints from TEX to XML, a format that simplifies downstream parsing. \n",
    "\n",
    "Each submission in the ./latex/ folder should contain 1 or more .tex files. If the submission contains more than 1 .tex file, we identify the main file. The additional files are usually inserts for the main file. \n",
    "\n",
    "After collecting the filepaths for all submissions' main files, we convert them from .tex to .xml using the [latexml](https://dlmf.nist.gov/LaTeXML/) package, spreading the work across all CPU cores (4 on my machine).\n",
    "\n",
    "All converted .xml files are stored in ./xml/.\n",
    "\n",
    "## 2. Extract and convert preprints\n",
    "\n",
    "Import all dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, subprocess, glob, multiprocessing, time, tarfile, gzip, shutil, zipfile\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grab arXiv submission identifiers from the metadata, and load conversion log:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>submission</th>\n",
       "      <th>tarfile</th>\n",
       "      <th>type</th>\n",
       "      <th>extracted</th>\n",
       "      <th>extracted_suffix</th>\n",
       "      <th>converted</th>\n",
       "      <th>conversion_result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>astro-ph0001001</td>\n",
       "      <td>arXiv_src_0001_001</td>\n",
       "      <td>.gz</td>\n",
       "      <td>yes</td>\n",
       "      <td>.zip</td>\n",
       "      <td>yes</td>\n",
       "      <td>\\n(Loading /opt/local/lib/perl5/vendor_perl/5....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>astro-ph0001002</td>\n",
       "      <td>arXiv_src_0001_001</td>\n",
       "      <td>.gz</td>\n",
       "      <td>yes</td>\n",
       "      <td>.zip</td>\n",
       "      <td>yes</td>\n",
       "      <td>\\n(Loading /opt/local/lib/perl5/vendor_perl/5....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>astro-ph0001003</td>\n",
       "      <td>arXiv_src_0001_001</td>\n",
       "      <td>.gz</td>\n",
       "      <td>yes</td>\n",
       "      <td>.zip</td>\n",
       "      <td>yes</td>\n",
       "      <td>\\n(Loading /opt/local/lib/perl5/vendor_perl/5....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>astro-ph0001004</td>\n",
       "      <td>arXiv_src_0001_001</td>\n",
       "      <td>.gz</td>\n",
       "      <td>yes</td>\n",
       "      <td>.zip</td>\n",
       "      <td>yes</td>\n",
       "      <td>\\n(Loading /opt/local/lib/perl5/vendor_perl/5....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>astro-ph0001005</td>\n",
       "      <td>arXiv_src_0001_001</td>\n",
       "      <td>.gz</td>\n",
       "      <td>yes</td>\n",
       "      <td>.zip</td>\n",
       "      <td>no</td>\n",
       "      <td>\\n(Loading /opt/local/lib/perl5/vendor_perl/5....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31286</td>\n",
       "      <td>nucl-th0401052</td>\n",
       "      <td>arXiv_src_0401_001</td>\n",
       "      <td>.gz</td>\n",
       "      <td>yes</td>\n",
       "      <td>.zip</td>\n",
       "      <td>yes</td>\n",
       "      <td>\\n(Loading /opt/local/lib/perl5/vendor_perl/5....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31287</td>\n",
       "      <td>nucl-th0401054</td>\n",
       "      <td>arXiv_src_0401_001</td>\n",
       "      <td>.gz</td>\n",
       "      <td>yes</td>\n",
       "      <td>.zip</td>\n",
       "      <td>yes</td>\n",
       "      <td>\\n(Loading /opt/local/lib/perl5/vendor_perl/5....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31288</td>\n",
       "      <td>physics0401131</td>\n",
       "      <td>arXiv_src_0401_001</td>\n",
       "      <td>.pdf</td>\n",
       "      <td>no</td>\n",
       "      <td>.zip</td>\n",
       "      <td>no</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31289</td>\n",
       "      <td>physics0401163</td>\n",
       "      <td>arXiv_src_0401_001</td>\n",
       "      <td>.gz</td>\n",
       "      <td>yes</td>\n",
       "      <td>.zip</td>\n",
       "      <td>yes</td>\n",
       "      <td>\\n(Loading /opt/local/lib/perl5/vendor_perl/5....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31290</td>\n",
       "      <td>q-bio0401036</td>\n",
       "      <td>arXiv_src_0401_001</td>\n",
       "      <td>.gz</td>\n",
       "      <td>yes</td>\n",
       "      <td>.zip</td>\n",
       "      <td>yes</td>\n",
       "      <td>\\n(Loading /opt/local/lib/perl5/vendor_perl/5....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>31291 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            submission             tarfile  type extracted extracted_suffix  \\\n",
       "0      astro-ph0001001  arXiv_src_0001_001   .gz       yes             .zip   \n",
       "1      astro-ph0001002  arXiv_src_0001_001   .gz       yes             .zip   \n",
       "2      astro-ph0001003  arXiv_src_0001_001   .gz       yes             .zip   \n",
       "3      astro-ph0001004  arXiv_src_0001_001   .gz       yes             .zip   \n",
       "4      astro-ph0001005  arXiv_src_0001_001   .gz       yes             .zip   \n",
       "...                ...                 ...   ...       ...              ...   \n",
       "31286   nucl-th0401052  arXiv_src_0401_001   .gz       yes             .zip   \n",
       "31287   nucl-th0401054  arXiv_src_0401_001   .gz       yes             .zip   \n",
       "31288   physics0401131  arXiv_src_0401_001  .pdf        no             .zip   \n",
       "31289   physics0401163  arXiv_src_0401_001   .gz       yes             .zip   \n",
       "31290     q-bio0401036  arXiv_src_0401_001   .gz       yes             .zip   \n",
       "\n",
       "      converted                                  conversion_result  \n",
       "0           yes  \\n(Loading /opt/local/lib/perl5/vendor_perl/5....  \n",
       "1           yes  \\n(Loading /opt/local/lib/perl5/vendor_perl/5....  \n",
       "2           yes  \\n(Loading /opt/local/lib/perl5/vendor_perl/5....  \n",
       "3           yes  \\n(Loading /opt/local/lib/perl5/vendor_perl/5....  \n",
       "4            no  \\n(Loading /opt/local/lib/perl5/vendor_perl/5....  \n",
       "...         ...                                                ...  \n",
       "31286       yes  \\n(Loading /opt/local/lib/perl5/vendor_perl/5....  \n",
       "31287       yes  \\n(Loading /opt/local/lib/perl5/vendor_perl/5....  \n",
       "31288        no                                                NaN  \n",
       "31289       yes  \\n(Loading /opt/local/lib/perl5/vendor_perl/5....  \n",
       "31290       yes  \\n(Loading /opt/local/lib/perl5/vendor_perl/5....  \n",
       "\n",
       "[31291 rows x 7 columns]"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata_df = pd.read_csv('../data/2020_03_06_arxiv_metadata_astroph/arxiv_metadata_astroph.csv', \n",
    "                         dtype={'filename_parsed': str})\n",
    "identifiers = list(metadata_df['filename_parsed'])\n",
    "log_df = pd.read_csv('../data/2020_03_09_extract_and_convert_submissions/conversion_log.csv')\n",
    "log_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define utility functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Astrophysics submissions: 267794\n",
      "Tar directories: 2\n",
      "Tars/tasks: 2503\n",
      "Submissions processed: 31291\n"
     ]
    }
   ],
   "source": [
    "extraction_path = '../data/2020_03_09_extract_and_convert_submissions/temp_extractions/'\n",
    "csv = '../data/2020_03_09_extract_and_convert_submissions/conversion_log.csv'\n",
    "\n",
    "print('Astrophysics submissions: ' + str(len(identifiers)))\n",
    "\n",
    "tar_dir = '../data/2020_03_03_original_arxiv_tars/'\n",
    "tar_dirs = ['../data/2020_03_03_original_arxiv_tars/', '../data/2020_03_07_update_tars/']\n",
    "print('Tar directories: ' + str(len(tar_dirs)))\n",
    "\n",
    "tasks = []\n",
    "for tar_dir in tar_dirs: \n",
    "    tasks += [tar_dir + x for x in os.listdir(tar_dir) if x.endswith('.tar')]\n",
    "print('Tars/tasks: ' + str(len(tasks)))\n",
    "\n",
    "\n",
    "print('Submissions processed: ' + str(len(log_df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "854"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks = np.array_split(tasks, 3) # do this in both notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_outpath(submission_path):\n",
    "    '''\n",
    "    Returns the filepath for a XML file,\n",
    "    based on the given filepath. \n",
    "    '''\n",
    "    submission_id = os.path.splitext(os.path.basename(submission_path))[0]\n",
    "    outpath = '../data/2020_03_09_extract_and_convert_submissions/converted_xml/' + submission_id + '.xml'\n",
    "    return outpath\n",
    "\n",
    "def open_tar(tar_file):\n",
    "    if os.path.isfile(tar_file) and tarfile.is_tarfile(tar_file):\n",
    "        tar = tarfile.open(tar_file)\n",
    "        tar_name = os.path.splitext(os.path.basename(tar.name))[0]\n",
    "        return tar, tar_name \n",
    "\n",
    "\n",
    "def extract(tar, submission, extracted_gz_path):\n",
    "    '''\n",
    "    Extracts given submission (formatted as TarInfo object).\n",
    "    Returns string signifying whether or not extraction was successful.\n",
    "    '''\n",
    "    \n",
    "    try:\n",
    "        suffix = '.zip'\n",
    "        gz_obj = tar.extractfile(submission)\n",
    "        gz = tarfile.open(fileobj=gz_obj, mode='r|gz')\n",
    "        zipf = zipfile.ZipFile(file=extracted_gz_path + suffix, mode='a', compression=zipfile.ZIP_DEFLATED)\n",
    "\n",
    "        for m in gz:\n",
    "            f = gz.extractfile(m)\n",
    "            if m.isdir():\n",
    "                continue\n",
    "            f_out = f.read()\n",
    "            f_in = m.name\n",
    "            zipf.writestr(f_in, f_out)\n",
    "        zipf.close()\n",
    "        gz.close()\n",
    "        extracted = 'yes'\n",
    "    except tarfile.ReadError: \n",
    "        # These submissions contain a single .tex file with no extension,\n",
    "        # so we need to treat them differently\n",
    "        suffix = '.tex'\n",
    "        tar.extract(submission, extraction_path)\n",
    "        with gzip.open(extraction_path + submission.name, 'rb') as f_in:\n",
    "            with open(extraction_path + submission.name + suffix, 'wb+') as f_out:\n",
    "                shutil.copyfileobj(f_in, f_out)\n",
    "        extracted = 'yes'\n",
    "        \n",
    "    return extracted, suffix\n",
    "\n",
    "\n",
    "def do_not_convert(submission_id):\n",
    "    '''\n",
    "    Returns boolean indicating whether or not \n",
    "    the given submission should be converted.\n",
    "    We only want submissions that are from the\n",
    "    astrophysics archive, and, for performance\n",
    "    purposes, that haven't already been converted.\n",
    "    '''\n",
    "    \n",
    "    not_astrophysics = submission_id not in identifiers\n",
    "    already_converted = submission_id in log_df['submission'].values\n",
    "    return not_astrophysics or already_converted \n",
    "\n",
    "        \n",
    "def convert(submission_path):\n",
    "    '''\n",
    "    Converts file at passed filepath to XML,\n",
    "    using LaTeXMLc.\n",
    "    '''\n",
    "    outpath = get_outpath(submission_path)\n",
    "    print('Converting to ' + outpath)\n",
    "    try:\n",
    "        proc = subprocess.Popen(['latexmlc', '--timeout=240', '--dest=' + outpath, submission_path], stderr=subprocess.PIPE)\n",
    "        out, err = proc.communicate()\n",
    "        err = err.decode('utf-8')\n",
    "        # Check if file was converted successfully\n",
    "        if 'Error! Did not write file' in err:\n",
    "            converted = 'no'\n",
    "        else:\n",
    "            converted = 'yes'\n",
    "    except Exception as e:\n",
    "        print('Something went wrong in convert(): ' + str(e))\n",
    "        converted = 'no'\n",
    "    return err, converted\n",
    "\n",
    "def save_log(log):\n",
    "    # Save log so we have something if it fails\n",
    "    df = pd.DataFrame(log, columns=['submission', \n",
    "                            'tarfile', \n",
    "                            'type', \n",
    "                            'extracted', \n",
    "                            'extracted_suffix',\n",
    "                            'converted',\n",
    "                            'conversion_result'])\n",
    "    df.to_csv(csv, mode='a', header=(not os.path.exists(csv)), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def work(task):\n",
    "    '''\n",
    "    Defines work to be done for each tar.\n",
    "    task — filepath to tar\n",
    "    '''\n",
    "    \n",
    "    # Open it as read-only\n",
    "    log = []\n",
    "    tar, tar_name = open_tar(task)\n",
    "    print('\\nOpening ' + tar_name + ',')\n",
    "    extracted_tar_path = None # temp, only for ReadErrors, will know path once we get tar contents\n",
    "\n",
    "    # For each submission (.gz or .pdf)\n",
    "    for submission in tar.getmembers():\n",
    "        if submission.name.endswith('.gz') or submission.name.endswith('.pdf'):\n",
    "            submission_id = os.path.splitext(os.path.basename(submission.name))[0]\n",
    "            if do_not_convert(submission_id):\n",
    "                continue\n",
    "            print('Working on submission: ' + submission_id + '...')\n",
    "            submission_path = os.path.splitext(task)[0] + '/' + submission_id\n",
    "            submission_type = os.path.splitext(os.path.basename(submission.name))[1]\n",
    "\n",
    "            # If .pdf, skip, we cannot extract and will not convert here\n",
    "            if submission_type == '.pdf':\n",
    "                result = None\n",
    "                extracted = 'no'\n",
    "                converted = 'no'\n",
    "                suffix = '' # WILL NEED TO GO BACK THROUGH AND FIX BECAUSE I ADDED THIS LATE\n",
    "            else:\n",
    "                # Extract submission \n",
    "                #extracted_gz_path = '../data/2020_03_09_extract_and_convert_submissions/temp_extractions/'\n",
    "                extracted_tar_path = extraction_path + submission.name.split('/')[0] + '/'\n",
    "                #print('Extracted gz path: ' + extracted_gz_path)\n",
    "                #print('Extracted tar path: ' + extracted_tar_path)\n",
    "                extracted, suffix = extract(tar, submission, extraction_path + submission_id)\n",
    "\n",
    "                print(extracted + ' ' + suffix)\n",
    "                # Convert submission and remove extracted submission\n",
    "                if extracted == 'yes':\n",
    "                    result, converted = convert(extraction_path + submission_id + suffix)\n",
    "                \n",
    "                # Remove the folder that appears for the tar during ReadErrors, if it exists\n",
    "                if os.path.exists(extracted_tar_path): \n",
    "                    shutil.rmtree(extracted_tar_path)\n",
    "                else:\n",
    "                    os.remove(extraction_path + submission_id + suffix)\n",
    "\n",
    "            # Log submission extraction & conversion info, remove .zip\n",
    "            log.append([submission_id, tar_name, submission_type, extracted, suffix, converted, result])\n",
    "\n",
    "    save_log(log)\n",
    "\n",
    "    # After finishing tarfile, move it to 'processed' directory\n",
    "    os.rename(task, os.path.dirname(task) + '/processed/' + tar_name + '.tar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Opening arXiv_src_0402_001,\n",
      "Working on submission: astro-ph0402001...\n",
      "Working on submission: astro-ph0402002...\n",
      "yes .zip\n",
      "Converting to ../data/2020_03_09_extract_and_convert_submissions/converted_xml/astro-ph0402002.xml\n",
      "Working on submission: astro-ph0402003...\n",
      "yes .zip\n",
      "Converting to ../data/2020_03_09_extract_and_convert_submissions/converted_xml/astro-ph0402003.xml\n"
     ]
    }
   ],
   "source": [
    "for task in tasks:\n",
    "    work(task)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. analyze"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get from Untitled.ipynb then delete Untitled.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Can retrieve individual preprints from online or from the associated tar file:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This helped: https://jarrodmcclean.com/simple-bash-parallel-commands-in-python/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First I need to confirm the main file in each repository. \n",
    "- If it doesn't contain a .bbl file, I need to add it to the bbl_lack folder. Later. Set aside and skip.\n",
    "- If it doesn't contain a file, I need to retrieve it again. Later. Set aside and skip. \n",
    "\n",
    "I will look at each submission folder, check xml to see if a file exists with its name. If not, I will go into the submission folder to check each file if it contains \\\\documentclass. If it does, grab it and convert it. Break out of loop. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 1010.3382.tar.gz\n"
     ]
    }
   ],
   "source": [
    "def guess_extension_from_headers(h):\n",
    "    \"\"\"\n",
    "    Given headers from an ArXiV e-print response, try and guess what the file\n",
    "    extension should be.\n",
    "    Based on: https://arxiv.org/help/mimetypes\n",
    "    \"\"\"\n",
    "    if h.get('content-type') == 'application/pdf':\n",
    "        return '.pdf'\n",
    "    if h.get('content-encoding') == 'x-gzip' and h.get('content-type') == 'application/postscript':\n",
    "        return '.ps.gz'\n",
    "    if h.get('content-encoding') == 'x-gzip' and h.get('content-type') == 'application/x-eprint-tar':\n",
    "        return '.tar.gz'\n",
    "    # content-encoding is x-gzip but this appears to normally be a lie - it's\n",
    "    # just plain text\n",
    "    if h.get('content-type') == 'application/x-eprint':\n",
    "        return '.tex'\n",
    "    if h.get('content-encoding') == 'x-gzip' and h.get('content-type') == 'application/x-dvi':\n",
    "        return '.dvi.gz'\n",
    "    return None\n",
    "\n",
    "def arxiv_id_to_source_url(arxiv_id):\n",
    "    # This URL is normally a tarball, but sometimes something else.\n",
    "    # ArXiV provides a /src/ URL which always serves up a tarball,\n",
    "    # but if we used this, we'd have to untar the file to figure out\n",
    "    # whether it's renderable or not. By using the /e-print/ endpoint\n",
    "    # we can figure out straight away whether we should bother rendering\n",
    "    # it or not.\n",
    "    # https://arxiv.org/help/mimetypes has more info\n",
    "    return 'https://arxiv.org/e-print/' + arxiv_id\n",
    "\n",
    "def download_source_file(arxiv_id):\n",
    "    \"\"\"\n",
    "    Download the LaTeX source of this paper and returns as ContentFile.\n",
    "    \"\"\"\n",
    "    source_url = arxiv_id_to_source_url(arxiv_id)\n",
    "    res = requests.get(source_url)\n",
    "    res.raise_for_status()\n",
    "    extension = guess_extension_from_headers(res.headers)\n",
    "    if not extension:\n",
    "        raise DownloadError(\"Could not determine file extension from \"\n",
    "                            \"headers: Content-Type: {}; \"\n",
    "                            \"Content-Encoding: {}\".format(\n",
    "                                res.headers.get('content-type'),\n",
    "                                res.headers.get('content-encoding')))\n",
    "    with open(arxiv_id + extension, 'wb+') as f:\n",
    "        f.write(res.content)\n",
    "        print('Created ' + arxiv_id + extension)\n",
    "\n",
    "download_source_file('1010.3382')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
