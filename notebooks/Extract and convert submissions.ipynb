{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert preprints \n",
    "\n",
    "Contents:\n",
    "1. Introduction\n",
    "2. Extract and convert preprints\n",
    "2. Identify preprints\n",
    "3. Convert preprints\n",
    "4. Appendix: Request a submission from arXiv API\n",
    "\n",
    "## 1. Introduction \n",
    "\n",
    "In this notebook, we convert preprints from TEX to XML, a format that simplifies downstream parsing. \n",
    "\n",
    "Each submission in the ./latex/ folder should contain 1 or more .tex files. If the submission contains more than 1 .tex file, we identify the main file. The additional files are usually inserts for the main file. \n",
    "\n",
    "After collecting the filepaths for all submissions' main files, we convert them from .tex to .xml using the [latexml](https://dlmf.nist.gov/LaTeXML/) package, spreading the work across all CPU cores (4 on my machine).\n",
    "\n",
    "All converted .xml files are stored in ./xml/.\n",
    "\n",
    "## 2. Extract and convert preprints\n",
    "\n",
    "Import all dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, subprocess, glob, multiprocessing, time, pathlib, tarfile, gzip, shutil, zipfile\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grab arXiv submission identifiers from the metadata:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/briennakh/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3063: DtypeWarning: Columns (1) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "267794"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata_df = pd.read_csv('../data/2020_03_06_arxiv_metadata_astroph/arxiv_metadata_astroph.csv', \n",
    "                         dtype={'filename_parsed': str})\n",
    "identifiers = list(metadata_df['filename_parsed'])\n",
    "len(identifiers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load conversion log:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>submission</th>\n",
       "      <th>tarfile</th>\n",
       "      <th>type</th>\n",
       "      <th>extracted</th>\n",
       "      <th>extracted_suffix</th>\n",
       "      <th>converted</th>\n",
       "      <th>conversion_result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>astro-ph0001001</td>\n",
       "      <td>arXiv_src_0001_001</td>\n",
       "      <td>.gz</td>\n",
       "      <td>yes</td>\n",
       "      <td>.zip</td>\n",
       "      <td>yes</td>\n",
       "      <td>\\n(Loading /opt/local/lib/perl5/vendor_perl/5....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>astro-ph0001002</td>\n",
       "      <td>arXiv_src_0001_001</td>\n",
       "      <td>.gz</td>\n",
       "      <td>yes</td>\n",
       "      <td>.zip</td>\n",
       "      <td>yes</td>\n",
       "      <td>\\n(Loading /opt/local/lib/perl5/vendor_perl/5....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>astro-ph0001003</td>\n",
       "      <td>arXiv_src_0001_001</td>\n",
       "      <td>.gz</td>\n",
       "      <td>yes</td>\n",
       "      <td>.zip</td>\n",
       "      <td>yes</td>\n",
       "      <td>\\n(Loading /opt/local/lib/perl5/vendor_perl/5....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>astro-ph0001004</td>\n",
       "      <td>arXiv_src_0001_001</td>\n",
       "      <td>.gz</td>\n",
       "      <td>yes</td>\n",
       "      <td>.zip</td>\n",
       "      <td>yes</td>\n",
       "      <td>\\n(Loading /opt/local/lib/perl5/vendor_perl/5....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>astro-ph0001005</td>\n",
       "      <td>arXiv_src_0001_001</td>\n",
       "      <td>.gz</td>\n",
       "      <td>yes</td>\n",
       "      <td>.zip</td>\n",
       "      <td>no</td>\n",
       "      <td>\\n(Loading /opt/local/lib/perl5/vendor_perl/5....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12527</td>\n",
       "      <td>nlin0109004</td>\n",
       "      <td>arXiv_src_0109_001</td>\n",
       "      <td>.gz</td>\n",
       "      <td>yes</td>\n",
       "      <td>.zip</td>\n",
       "      <td>yes</td>\n",
       "      <td>\\n(Loading /opt/local/lib/perl5/vendor_perl/5....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12528</td>\n",
       "      <td>nlin0109028</td>\n",
       "      <td>arXiv_src_0109_001</td>\n",
       "      <td>.gz</td>\n",
       "      <td>yes</td>\n",
       "      <td>.zip</td>\n",
       "      <td>yes</td>\n",
       "      <td>\\n(Loading /opt/local/lib/perl5/vendor_perl/5....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12529</td>\n",
       "      <td>nucl-th0109009</td>\n",
       "      <td>arXiv_src_0109_001</td>\n",
       "      <td>.gz</td>\n",
       "      <td>yes</td>\n",
       "      <td>.zip</td>\n",
       "      <td>yes</td>\n",
       "      <td>\\n(Loading /opt/local/lib/perl5/vendor_perl/5....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12530</td>\n",
       "      <td>physics0109018</td>\n",
       "      <td>arXiv_src_0109_001</td>\n",
       "      <td>.gz</td>\n",
       "      <td>no</td>\n",
       "      <td>.tex</td>\n",
       "      <td>yes</td>\n",
       "      <td>\\n(Loading /opt/local/lib/perl5/vendor_perl/5....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12531</td>\n",
       "      <td>physics0109072</td>\n",
       "      <td>arXiv_src_0109_001</td>\n",
       "      <td>.pdf</td>\n",
       "      <td>no</td>\n",
       "      <td>NaN</td>\n",
       "      <td>no</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12532 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            submission             tarfile  type extracted extracted_suffix  \\\n",
       "0      astro-ph0001001  arXiv_src_0001_001   .gz       yes             .zip   \n",
       "1      astro-ph0001002  arXiv_src_0001_001   .gz       yes             .zip   \n",
       "2      astro-ph0001003  arXiv_src_0001_001   .gz       yes             .zip   \n",
       "3      astro-ph0001004  arXiv_src_0001_001   .gz       yes             .zip   \n",
       "4      astro-ph0001005  arXiv_src_0001_001   .gz       yes             .zip   \n",
       "...                ...                 ...   ...       ...              ...   \n",
       "12527      nlin0109004  arXiv_src_0109_001   .gz       yes             .zip   \n",
       "12528      nlin0109028  arXiv_src_0109_001   .gz       yes             .zip   \n",
       "12529   nucl-th0109009  arXiv_src_0109_001   .gz       yes             .zip   \n",
       "12530   physics0109018  arXiv_src_0109_001   .gz        no             .tex   \n",
       "12531   physics0109072  arXiv_src_0109_001  .pdf        no              NaN   \n",
       "\n",
       "      converted                                  conversion_result  \n",
       "0           yes  \\n(Loading /opt/local/lib/perl5/vendor_perl/5....  \n",
       "1           yes  \\n(Loading /opt/local/lib/perl5/vendor_perl/5....  \n",
       "2           yes  \\n(Loading /opt/local/lib/perl5/vendor_perl/5....  \n",
       "3           yes  \\n(Loading /opt/local/lib/perl5/vendor_perl/5....  \n",
       "4            no  \\n(Loading /opt/local/lib/perl5/vendor_perl/5....  \n",
       "...         ...                                                ...  \n",
       "12527       yes  \\n(Loading /opt/local/lib/perl5/vendor_perl/5....  \n",
       "12528       yes  \\n(Loading /opt/local/lib/perl5/vendor_perl/5....  \n",
       "12529       yes  \\n(Loading /opt/local/lib/perl5/vendor_perl/5....  \n",
       "12530       yes  \\n(Loading /opt/local/lib/perl5/vendor_perl/5....  \n",
       "12531        no                                                NaN  \n",
       "\n",
       "[12532 rows x 7 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_df = pd.read_csv('../data/2020_03_09_extract_and_convert_submissions/conversion_log.csv')\n",
    "log_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define utility functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tar directories to process: 2\n",
      "Submissions already converted: 15510\n",
      "This program will run on 3 cores.\n"
     ]
    }
   ],
   "source": [
    "tar_dirs = ['../data/2020_03_03_original_arxiv_tars/', '../data/2020_03_07_update_tars/']\n",
    "print('Number of tar directories to process: ' + str(len(tar_dirs)))\n",
    "\n",
    "converted_submissions = [os.path.splitext(os.path.basename(x))[0] for x in os.listdir('../data/2020_03_09_extract_and_convert_submissions/converted_xml/')]\n",
    "print('Submissions already converted: ' + str(len(converted_submissions)))\n",
    "\n",
    "processes = multiprocessing.cpu_count() - 1\n",
    "print('This program will run on ' + str(processes) + ' cores.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_outpath(inpath):\n",
    "    '''\n",
    "    Returns the filepath for a XML file,\n",
    "    based on the given filepath. \n",
    "    '''\n",
    "    path_parts = pathlib.Path(inpath).parts\n",
    "    submission_id = os.path.splitext(path_parts[4])[0]\n",
    "    outpath = '../data/2020_03_09_extract_and_convert_submissions/converted_xml/' + submission_id + '.xml'\n",
    "    return outpath, submission_id\n",
    "\n",
    "\n",
    "def extract(submission):\n",
    "    '''\n",
    "    Extracts given submission (formatted as TarInfo object).\n",
    "    Returns string signifying whether or not extraction was successful.\n",
    "    '''\n",
    "    \n",
    "    try:\n",
    "        suffix = '.zip'\n",
    "        gz_obj = tar.extractfile(submission)\n",
    "        gz = tarfile.open(fileobj=gz_obj, mode='r|gz')\n",
    "        zipf = zipfile.ZipFile(file=extraction_path + submission_id + suffix, mode='a', compression=zipfile.ZIP_DEFLATED)\n",
    "\n",
    "        for m in gz:\n",
    "            f = gz.extractfile(m)\n",
    "            if m.isdir():\n",
    "                continue\n",
    "            f_out = f.read()\n",
    "            f_in = m.name\n",
    "            zipf.writestr(f_in, f_out)\n",
    "        zipf.close()\n",
    "        gz.close()\n",
    "        extracted = 'yes'\n",
    "\n",
    "    except tarfile.ReadError: \n",
    "        # These submissions contain a single .tex file with no extension,\n",
    "        # so we need to treat them differently\n",
    "        suffix = '.tex'\n",
    "        tar.extract(submission, extraction_path)\n",
    "        with gzip.open(extraction_path + submission.name, 'rb') as f_in:\n",
    "            with open(extraction_path + submission_id + suffix, 'wb+') as f_out:\n",
    "                    shutil.copyfileobj(f_in, f_out)\n",
    "        extracted = 'yes'\n",
    "\n",
    "    except Exception as e:\n",
    "        print('Something went wrong in convert(): ' + str(e))\n",
    "        extracted = 'no'\n",
    "        \n",
    "    return extracted, suffix\n",
    "\n",
    "\n",
    "def do_not_convert(submission_id):\n",
    "    '''\n",
    "    Returns boolean indicating whether or not \n",
    "    the given submission should be converted.\n",
    "    We only want submissions that are from the\n",
    "    astrophysics archive, and, for performance\n",
    "    purposes, that haven't already been converted.\n",
    "    '''\n",
    "    \n",
    "    not_astrophysics = submission_id not in identifiers\n",
    "    already_converted = submission_id in log_df['submission'].values\n",
    "    conversion_attempted = submission_id in converted_submissions # catches ones that failed, we don't want to try them again\n",
    "    return not_astrophysics or already_converted or conversion_attempted\n",
    "\n",
    "        \n",
    "def convert(submission_path):\n",
    "    '''\n",
    "    Converts file at passed filepath to XML,\n",
    "    using LaTeXMLc.\n",
    "    '''\n",
    "    outpath, submission_id = get_outpath(submission_path)\n",
    "    try:\n",
    "        print('Converting ' + submission_path + '...')\n",
    "        proc = sp.Popen(['latexmlc', '--timeout=240', '--dest=' + outpath, submission_path], stderr=subprocess.PIPE)\n",
    "        out, err = proc.communicate()\n",
    "        err = err.decode('utf-8')\n",
    "        # Check if file was converted successfully\n",
    "        if 'Error! Did not write file' in err:\n",
    "            converted = 'no'\n",
    "        else:\n",
    "            converted = 'yes'\n",
    "    except Exception as e:\n",
    "        print('Something went wrong in convert(): ' + str(e))\n",
    "        converted = 'no'\n",
    "    return err, converted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening arXiv_src_0203_001.tar,\n",
      "Working on submission: astro-ph0203078...\n",
      "Converting ../data/2020_03_09_extract_and_convert_submissions/temp/astro-ph0203078.zip...\n",
      "Working on submission: astro-ph0203113...\n",
      "Working on submission: astro-ph0203140...\n",
      "Working on submission: astro-ph0203212...\n",
      "Working on submission: astro-ph0203231...\n",
      "Converting ../data/2020_03_09_extract_and_convert_submissions/temp/astro-ph0203231.zip...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/briennakh/anaconda3/lib/python3.6/zipfile.py:1355: UserWarning: Duplicate name: 'aa.cls'\n",
      "  return self._open_to_write(zinfo, force_zip64=force_zip64)\n",
      "/Users/briennakh/anaconda3/lib/python3.6/zipfile.py:1355: UserWarning: Duplicate name: 'h3270.tex'\n",
      "  return self._open_to_write(zinfo, force_zip64=force_zip64)\n",
      "/Users/briennakh/anaconda3/lib/python3.6/zipfile.py:1355: UserWarning: Duplicate name: 'h3270f1.eps'\n",
      "  return self._open_to_write(zinfo, force_zip64=force_zip64)\n",
      "/Users/briennakh/anaconda3/lib/python3.6/zipfile.py:1355: UserWarning: Duplicate name: 'h3270f2.eps'\n",
      "  return self._open_to_write(zinfo, force_zip64=force_zip64)\n",
      "/Users/briennakh/anaconda3/lib/python3.6/zipfile.py:1355: UserWarning: Duplicate name: 'h3270f3.eps'\n",
      "  return self._open_to_write(zinfo, force_zip64=force_zip64)\n",
      "/Users/briennakh/anaconda3/lib/python3.6/zipfile.py:1355: UserWarning: Duplicate name: 'h3270f4.eps'\n",
      "  return self._open_to_write(zinfo, force_zip64=force_zip64)\n",
      "/Users/briennakh/anaconda3/lib/python3.6/zipfile.py:1355: UserWarning: Duplicate name: 'h3270f5.eps'\n",
      "  return self._open_to_write(zinfo, force_zip64=force_zip64)\n",
      "/Users/briennakh/anaconda3/lib/python3.6/zipfile.py:1355: UserWarning: Duplicate name: 'h3270f6.eps'\n",
      "  return self._open_to_write(zinfo, force_zip64=force_zip64)\n",
      "/Users/briennakh/anaconda3/lib/python3.6/zipfile.py:1355: UserWarning: Duplicate name: 'h3270f7.eps'\n",
      "  return self._open_to_write(zinfo, force_zip64=force_zip64)\n",
      "/Users/briennakh/anaconda3/lib/python3.6/zipfile.py:1355: UserWarning: Duplicate name: 'h3270f8.eps'\n",
      "  return self._open_to_write(zinfo, force_zip64=force_zip64)\n",
      "/Users/briennakh/anaconda3/lib/python3.6/zipfile.py:1355: UserWarning: Duplicate name: 'h3270f9.eps'\n",
      "  return self._open_to_write(zinfo, force_zip64=force_zip64)\n",
      "/Users/briennakh/anaconda3/lib/python3.6/zipfile.py:1355: UserWarning: Duplicate name: 'natbib.sty'\n",
      "  return self._open_to_write(zinfo, force_zip64=force_zip64)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-64-ae55c3e76b5a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     42\u001b[0m                         \u001b[0;31m# Convert submission and remove extracted submission\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0mextracted\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'yes'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m                             \u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mextraction_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msubmission_id\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msuffix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m                             \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mextraction_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msubmission_id\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msuffix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-63-db2eaa1506ce>\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(submission_path)\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Converting '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msubmission_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0mproc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'latexmlc'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'--timeout=240'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'--dest='\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0moutpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubmission_path\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPIPE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m         \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mproc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommunicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m         \u001b[0merr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0;31m# Check if file was converted successfully\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/subprocess.py\u001b[0m in \u001b[0;36mcommunicate\u001b[0;34m(self, input, timeout)\u001b[0m\n\u001b[1;32m    831\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 833\u001b[0;31m                 \u001b[0mstderr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    834\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# For each tar directory,\n",
    "for tar_dir in tar_dirs: \n",
    "    files = os.listdir(tar_dir)\n",
    "    \n",
    "    # For each tar file, \n",
    "    for file in files:\n",
    "        filepath = tar_dir + file\n",
    "        if os.path.isfile(filepath) and tarfile.is_tarfile(filepath):\n",
    "            \n",
    "            # Open it as read-only\n",
    "            log = []\n",
    "            print('Opening ' + file + ',')\n",
    "            tar = tarfile.open(filepath)\n",
    "            tar_name = os.path.splitext(os.path.basename(tar.name))[0]\n",
    "            extracted_tar_path = None # temp, only for ReadErrors, will know path once we get tar contents\n",
    "            \n",
    "            # Iterate over its .gz files (which are each an article submission),\n",
    "            for submission in tar.getmembers():\n",
    "                \n",
    "                # Only look at submissions (.gz and .pdf)\n",
    "                if submission.name.endswith('.gz') or submission.name.endswith('.pdf'):\n",
    "                    \n",
    "                    submission_id = os.path.splitext(os.path.basename(submission.name))[0]\n",
    "                    if do_not_convert(submission_id):\n",
    "                        continue\n",
    "                    \n",
    "                    print('Working on submission: ' + submission_id + '...')\n",
    "                    submission_path = tar_dir + os.path.splitext(os.path.basename(file))[0] + '/' + submission_id\n",
    "                    submission_type = os.path.splitext(os.path.basename(submission.name))[1]\n",
    "\n",
    "                    # If .pdf, skip, we cannot extract and will not convert here\n",
    "                    if submission_type == '.pdf':\n",
    "                        result = None\n",
    "                        extracted = 'no'\n",
    "                        converted = 'no'\n",
    "                    else:\n",
    "                        # Extract submission \n",
    "                        extraction_path = '../data/2020_03_09_extract_and_convert_submissions/temp/'\n",
    "                        extracted_tar_path = extraction_path + submission.name.split('/')[0]\n",
    "                        extracted, suffix = extract(submission)\n",
    "                        \n",
    "                        # Convert submission and remove extracted submission\n",
    "                        if extracted == 'yes':\n",
    "                            result, converted = convert(extraction_path + submission_id + suffix)\n",
    "                            os.remove(extraction_path + submission_id + suffix)\n",
    "                        \n",
    "                            # Remove the folder that appears for the tar during ReadErrors, if it exists\n",
    "                            if os.path.exists(extracted_tar_path): # IS THIS IN THE RIGHT PLACE???\n",
    "                                shutil.rmtree(extracted_tar_path)\n",
    "                        \n",
    "                    # Log submission extraction & conversion info, remove .zip\n",
    "                    log.append([submission_id, tar_name, submission_type, extracted, suffix, converted, result])\n",
    "            \n",
    "            # Save log so we have something if it fails \n",
    "            df = pd.DataFrame(log, columns=['submission', \n",
    "                                    'tarfile', \n",
    "                                    'type', \n",
    "                                    'extracted', \n",
    "                                    'extracted_suffix',\n",
    "                                    'converted',\n",
    "                                    'conversion_result'])\n",
    "            df.to_csv('../data/2020_03_09_extract_and_convert_submissions/conversion_log.csv', mode='a', header=not f.tell(), index=False)\n",
    "        \n",
    "            # After finishing tarfile, move it to 'processed' directory\n",
    "            if not os.path.isdir(tar_dir + 'processed'):\n",
    "                os.makedirs(tar_dir + 'processed')\n",
    "            os.rename(filepath, tar_dir + 'processed/' + file)\n",
    "            \n",
    "            print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. analyze"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get from Untitled.ipynb then delete Untitled.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Can retrieve individual preprints from online or from the associated tar file:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This helped: https://jarrodmcclean.com/simple-bash-parallel-commands-in-python/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First I need to confirm the main file in each repository. \n",
    "- If it doesn't contain a .bbl file, I need to add it to the bbl_lack folder. Later. Set aside and skip.\n",
    "- If it doesn't contain a file, I need to retrieve it again. Later. Set aside and skip. \n",
    "\n",
    "I will look at each submission folder, check xml to see if a file exists with its name. If not, I will go into the submission folder to check each file if it contains \\\\documentclass. If it does, grab it and convert it. Break out of loop. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 1010.3382.tar.gz\n"
     ]
    }
   ],
   "source": [
    "def guess_extension_from_headers(h):\n",
    "    \"\"\"\n",
    "    Given headers from an ArXiV e-print response, try and guess what the file\n",
    "    extension should be.\n",
    "    Based on: https://arxiv.org/help/mimetypes\n",
    "    \"\"\"\n",
    "    if h.get('content-type') == 'application/pdf':\n",
    "        return '.pdf'\n",
    "    if h.get('content-encoding') == 'x-gzip' and h.get('content-type') == 'application/postscript':\n",
    "        return '.ps.gz'\n",
    "    if h.get('content-encoding') == 'x-gzip' and h.get('content-type') == 'application/x-eprint-tar':\n",
    "        return '.tar.gz'\n",
    "    # content-encoding is x-gzip but this appears to normally be a lie - it's\n",
    "    # just plain text\n",
    "    if h.get('content-type') == 'application/x-eprint':\n",
    "        return '.tex'\n",
    "    if h.get('content-encoding') == 'x-gzip' and h.get('content-type') == 'application/x-dvi':\n",
    "        return '.dvi.gz'\n",
    "    return None\n",
    "\n",
    "def arxiv_id_to_source_url(arxiv_id):\n",
    "    # This URL is normally a tarball, but sometimes something else.\n",
    "    # ArXiV provides a /src/ URL which always serves up a tarball,\n",
    "    # but if we used this, we'd have to untar the file to figure out\n",
    "    # whether it's renderable or not. By using the /e-print/ endpoint\n",
    "    # we can figure out straight away whether we should bother rendering\n",
    "    # it or not.\n",
    "    # https://arxiv.org/help/mimetypes has more info\n",
    "    return 'https://arxiv.org/e-print/' + arxiv_id\n",
    "\n",
    "def download_source_file(arxiv_id):\n",
    "    \"\"\"\n",
    "    Download the LaTeX source of this paper and returns as ContentFile.\n",
    "    \"\"\"\n",
    "    source_url = arxiv_id_to_source_url(arxiv_id)\n",
    "    res = requests.get(source_url)\n",
    "    res.raise_for_status()\n",
    "    extension = guess_extension_from_headers(res.headers)\n",
    "    if not extension:\n",
    "        raise DownloadError(\"Could not determine file extension from \"\n",
    "                            \"headers: Content-Type: {}; \"\n",
    "                            \"Content-Encoding: {}\".format(\n",
    "                                res.headers.get('content-type'),\n",
    "                                res.headers.get('content-encoding')))\n",
    "    with open(arxiv_id + extension, 'wb+') as f:\n",
    "        f.write(res.content)\n",
    "        print('Created ' + arxiv_id + extension)\n",
    "\n",
    "download_source_file('1010.3382')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
